{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a825f3-5f44-4887-8257-59ff93fb1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT STATEMENTS \n",
    "import PyPDF2\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b4e75a-b7a6-463b-8210-d9c912956a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNSTION TO PRINT THE PROGRAM OUTPUTS IN THE CONSOLE\n",
    "import time\n",
    "def printer(v):\n",
    "    paragraphs = v.split('\\n')\n",
    "    for paragraph in paragraphs:\n",
    "        words = paragraph.split()\n",
    "        for word in words:\n",
    "            print(word, end=' ')\n",
    "            time.sleep(0.1)\n",
    "        print()\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bef9055-8c13-471a-acf0-c219fcd228d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION DEFINITONS\n",
    "\n",
    "# Function to extract text from TEXT/PDF\n",
    "def extract_text_from_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Function to print the output in a stream\n",
    "def output(stream):\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end=\"\")\n",
    "        response += chunk['message']['content']\n",
    "    return response\n",
    "\n",
    "# Function to extract and store the question paper\n",
    "def processone(q):\n",
    "    prompt1=f'''Extract the questions  from the provided text and check for grammatical and spelling mistakes. Correct any mistakes found and provide the clean, corrected text without any unnecessary comments or additional notes. Ensure the text flows naturally and is accurate.\n",
    "             *Part A (10 x 2 = 20 Marks):*\n",
    "     1. question\n",
    "     2. question\n",
    "     3. question\n",
    "     4. question\n",
    "     5. question\n",
    "     6. question\n",
    "     7. question\n",
    "     8. question\n",
    "     9. question\n",
    "     10. question\n",
    "\n",
    "     *Part B (5 x 13 = 65 Marks):*\n",
    "     11a. question\n",
    "     11b. question\n",
    "     12a. question\n",
    "     12b. question\n",
    "     13a. question\n",
    "     13b. question\n",
    "     14a. question\n",
    "     14b. question\n",
    "     15a. question\n",
    "     15b. question\n",
    "\n",
    "     *Part C (1 x 15 = 15 Marks):*\n",
    "     16a. question\n",
    "     16b. question\n",
    "\n",
    "Here is the text: {q}\n",
    "'''\n",
    "    stream = ollama.chat(\n",
    "        model='mistral',\n",
    "        messages=[{'role': 'user', 'content': prompt1}],\n",
    "        stream=True\n",
    "    )\n",
    "    result = output(stream)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Funtion to extract the irrelevant questions\n",
    "def check(question,syllabus):\n",
    "    prompt3 = f\"\"\"\n",
    "You are acting as a syllabus checker. Your task is to carefully review each questions and compare it with the syllabus content. Only questions that do not match the syllabus should be marked as irrelevant. Questions may come from a related subject, but the focus must be on exact syllabus alignment.\n",
    "\n",
    "Your Task:\n",
    "1. Compare each question with the syllabus topics, concepts, and learning objectives.\n",
    "2. If a question does not directly align with the syllabus, mark it as irrelevant, even if it shares some terminology with the subject.\n",
    "3. List only the irrelevant questions from the extracted list without any explanation or additional notes.\n",
    "\n",
    "Extracted Questions:\n",
    "{question}\n",
    "\n",
    "Syllabus Content:\n",
    "{syllabus}\n",
    "\n",
    "Instructions:\n",
    "- Be strict about comparing each question to the syllabus without assumptions based on related topics.\n",
    "- Output only the list of irrelevant questions in the simplest form, without comments or explanations along with their corresponding question number.\n",
    "\"\"\"\n",
    "\n",
    "    # Call Ollama mistral for processing\n",
    "    stream = ollama.chat(\n",
    "        model='mistral',\n",
    "        messages=[{'role': 'user', 'content': prompt3}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Get the result from the stream\n",
    "    result = output(stream)\n",
    "    return result\n",
    "\n",
    "# Function to generate relevant question to replace the irrelevant ones\n",
    "def generate_replacement_questions(irrelevant,syllabus):\n",
    "    prompt4 = f\"\"\"\n",
    "\n",
    "You are tasked with rewriting the following irrelevant questions to align them with the syllabus and follow Bloom's Taxonomy. Each rewritten question must retain its original question number for easy replacement.\n",
    "\n",
    "### Instructions:\n",
    "1. Rewrite each irrelevant question so it matches the topics and concepts in the syllabus.\n",
    "2. Maintain the original question numbers exactly as they are.\n",
    "3. Ensure the questions follow Bloomâ€™s Taxonomy:\n",
    "   - *Part A*: Lower-level cognitive skills (Remember, Understand).\n",
    "   - *Part B*: Mid-level cognitive skills (Apply, Analyze).\n",
    "   - *Part C*: Higher-level cognitive skills (Evaluate, Create).\n",
    "4. Ensure each question is unique and relevant to the syllabus.\n",
    "5. *Only provide the rewritten questions with their original question numbers.* No extra commentary is needed.\n",
    "\n",
    "### Example:\n",
    "- If question 5 is irrelevant, rewrite it like this:\n",
    "   5. [New question]\n",
    "\n",
    "### Irrelevant Questions:\n",
    "{irrelevant}\n",
    "\n",
    "### Syllabus Content:\n",
    "{syllabus}\n",
    "\n",
    "do not generate extra question. the number of replaced questions must be equal to the number of irrelevant questions\n",
    "\"\"\"\n",
    "# Use Ollama's model to process the prompt\n",
    "    stream = ollama.chat(\n",
    "        model='mistral',  # Using LLaMA 3.1 for accurate response generation\n",
    "        messages=[{'role': 'user', 'content': prompt4}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Get the result from the stream\n",
    "    result2 = output(stream)\n",
    "    return result2\n",
    "\n",
    "# Funtion to update the question paper with relevant question replacement\n",
    "def revised(question,irrelevant,replacement):\n",
    "    prompt5 = f\"\"\"\n",
    "Rewrite the question paper by replacing specific questions with new ones.\n",
    "\n",
    "Original question paper:\n",
    "{question}\n",
    "\n",
    "Replace the following questions:\n",
    "{irrelevant}\n",
    "\n",
    "With the following new questions:\n",
    "{replacement}\n",
    "\n",
    "Do not modify any questions that are not in the list of questions to replace.\n",
    "\"\"\"\n",
    "\n",
    "    # Use Ollama's model to process the prompt\n",
    "    stream = ollama.chat(\n",
    "        model='mistral',  # Using LLaMA 3.1 for accurate response generation\n",
    "        messages=[{'role': 'user', 'content': prompt5}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Get the result from the stream\n",
    "    result2 = output(stream)\n",
    "    return result2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feb5e7b-42d5-4e91-be2e-97ea6afcd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT EXTRACTION FROM THE SYLLABUS AND THE MODEL QUESTION PAPER\n",
    "\n",
    "q = extract_text_from_txt('model23.txt')\n",
    "s = extract_text_from_txt('ML1301.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d8a7c6d-744c-42b6-85f6-31ecfd16528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT TEXT FROM THE MODEL QUESTION PAPER\n",
    "question = processone(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d51b6d69-be80-4222-9f6b-739d26105026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the corrected text: \n",
      "\n",
      "*Part A (10 x 2 = 20 Marks):* \n",
      "1. What is a movie called Titanic? 2 CO1 BL1 \n",
      "2. What are the steps involved in the machine learning process? 2 CO1 BL1 \n",
      "3. When should Bies-vriance decomposition be used? 2 CO2 BL2 \n",
      "4. Differentiate between Discriminative and Generative models. 2 CO2 BL2 \n",
      "5. In what situations is PCA (Principal Component Analysis) suitable? 2 CO3 BL2 \n",
      "6. Explain how Linear Discriminant Analysis (LDA) works. 2 CO3 BL2 \n",
      "7. What are Transition Probabilities? 2 CO4 BL1 \n",
      "8. Define the HMM Viterbi algorithm. 2 CO4 BL1 \n",
      "9. Explain what a Perceptron is. 2 CO5 BL1 \n",
      "10. Explain what a Perceptron is. 2 CO5 BL1 \n",
      "\n",
      "*Part B (5 x 13 = 65 Marks):* \n",
      "11a. Write the FIND-S algorithm and provide an example to trace it. (13) CO1 BL1, BL2 \n",
      "OR \n",
      "11b. Provide an example to demonstrate the application of K-NN Algorithm \n",
      "and predict the sport that Angelina plays. \n",
      "\n",
      "12a. Given the attributes of life and competition, classify the activities (13) CO2 BL3, BL4 \n",
      "when you are deadlines looming, there is no party on, and you are lazy. \n",
      "OR \n",
      "12b. Using K-NN Algorithm, predict which sport Angelina plays belonging to? \n",
      "\n",
      "13a. Demonstrate the EM algorithm in detail and justify how it overcomes the drawback of K-means. (13) CO3 BL3 \n",
      "OR \n",
      "13b. Explain how Single-Linkage Hierarchical Clustering groups data samples using an example. \n",
      "\n",
      "14a. In a scenario where there are multiple deadlines but none urgent, find out the activity that should be happening? (13) CO4 BL4 \n",
      "OR \n",
      "14b. Given P(Hot)=0.8, P(Cold)=0.2.John eats 1 ice cream on first day, 3 ice cream on second day, 1 ice cream on third day. Infer the hidden states? \n",
      "\n",
      "15a. Illustrate Ensemble Learning with an example and its application. (13) CO5 BL3 \n",
      "OR \n",
      "15b. Explain the Multi-Layer Perceptron algorithm and justify how it solves the XOR problem. \n",
      "\n",
      "*Part C (1 x 15 = 15 Marks):* \n",
      "16a. Given the data in Table, Estimate the Eigen Values and reduce the dimension from 2 to 1 using the Principal Component Analysis (PCA) algorithm. (15) CO3 BL5, BL6 \n",
      "OR \n",
      "16b. Explain how to evaluate a Support Vector Machine (SVM) model and create a hyperplane for classifying data samples. \n"
     ]
    }
   ],
   "source": [
    "#QUESTION\n",
    "printer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75b36914-dba2-4610-ab52-42a0c45e532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT THE CONTENT OF THE SYLLABUS\n",
    "# Function to understand the syllabus\n",
    "def understand(s):\n",
    "    prompt2=f'''summarise , what the syllabus {s} is about short and concise withing 5 or 10 sentences'''\n",
    "    # Call Ollama mistral for processing\n",
    "    stream = ollama.chat(\n",
    "        model='mistral',\n",
    "        messages=[{'role': 'user', 'content': prompt2}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Get the result from the stream\n",
    "    result = output(stream)\n",
    "    return result\n",
    "syllabus = understand(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52a67ebf-a4b3-4105-a456-f9aecee57b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course, ML1301 - DATA FOUNDATION, spans 45 periods. Its main objective is to acquire knowledge on Data Science and its Foundations. The curriculum covers topics like data process, organization, analysis, visualization, ethics, and recent trends in the field. \n",
      "\n",
      "Key areas of study include understanding different types of data (quantitative, qualitative), big data, metadata, distributed file systems, EDA philosophy, data structures, organizational models, extraction and ETL processes, spreadsheet manipulations, Tableau visualizations, ethics, and privacy issues in data science. \n",
      "\n",
      "Students will learn to explore the fundamental concepts of Data Science, understand the Data Science process and tools of EDA, address how Organizational structure influences efficiency and effectiveness, analyze and validate data using Spreadsheets and Tableau, and think through the ethics related to data sharing and decision-making. The course aims at building interactive dashboards for business purposes as well. \n",
      "\n",
      "Upon completion of this course, students will have a strong foundation in Data Science, with skills that can be applied in various industries. The course texts include Introducing Data Science by Davy Cielen et al., Ethics and Data Science by DJ Patil et al., and additional resources like Introduction to Machine Learning with Python and Getting Started with Tableau 2019.2. \n"
     ]
    }
   ],
   "source": [
    "#SYLLABUS\n",
    "printer(syllabus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22b830fa-49d0-4b67-a6f7-d7a05ae9478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK THE MODEL PAPER FOR IRRELEVANT QUESTIONS\n",
    "irrelevant = check(question, syllabus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36f5efe3-d4d1-44f5-86bc-926e29b30fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 4, 9, 10 (Part A) \n",
      "11b, 12a (Part B) \n",
      "14b, 15b (Part C) \n"
     ]
    }
   ],
   "source": [
    "#IRRELEVANT\n",
    "printer(irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45f06755-018c-4bec-8396-88c6cbaa177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE A SET OF REPLACEMENT QUESTIONS\n",
    "replacement = generate_replacement_questions(irrelevant,syllabus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2334ec5-df25-4c3f-b640-92388601e742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the primary purpose of data normalization in a database ? (Part A - Understand) \n",
      "\n",
      "2. Define the term 'quantitative data' within the context of Data Science. (Part A - Remember) \n",
      "\n",
      "4. Explain how big data differs from traditional data sets in terms of volume, variety, and velocity. (Part A - Understand) \n",
      "\n",
      "9. Apply the concepts learned about distributed file systems to design an efficient storage solution for a large dataset. (Part B - Apply) \n",
      "\n",
      "10. Analyze a given dataset using descriptive statistics to identify any outliers or trends, and interpret their implications. (Part B - Analyze) \n",
      "\n",
      "11b. Given a real-world scenario, suggest an appropriate data structure for storing and organizing the data based on its properties. (Part C - Evaluate) \n",
      "\n",
      "12a. Develop a hypothesis about the relationship between two variables in a given dataset, and design an experiment to test this hypothesis using EDA techniques. (Part C - Create) \n",
      "\n",
      "14b. Discuss the ethical implications of sharing sensitive data in Data Science, and propose solutions for mitigating potential privacy concerns. (Part C - Evaluate) \n",
      "\n",
      "15b. Design an interactive dashboard using Tableau 2019.2 that effectively visualizes a complex dataset for business decision-making purposes. (Part C - Create) \n"
     ]
    }
   ],
   "source": [
    "#REPLACEMENT\n",
    "printer(replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23ec8a89-d7da-46e3-a047-9521e14ff695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THE QUESTION PAPER WITH THE RELEVANT QUESTIONS\n",
    "def revised(question, irrelevant, replacement):\n",
    "    prompt5 = f\"\"\"\n",
    "Rewrite the question paper by substituting specific questions with new ones.\n",
    "\n",
    "Provide the original question paper content: {question}\n",
    "Specify the questions that need to be replaced: {irrelevant}\n",
    "Provide the new questions to replace them with: {replacement}\n",
    "\n",
    "Only modify the questions listed for replacement, leaving the rest of the question paper unchanged.\n",
    "MODIFY ALL THE QUESTIONS LISTED FOR REPLACEMENT WITHOUT FAIL!!\n",
    "\"\"\"\n",
    "\n",
    "    # Use Ollama's model to process the prompt\n",
    "    stream = ollama.chat(\n",
    "        model='mistral',  # Using LLaMA 3.1 for accurate response generation\n",
    "        messages=[{'role': 'user', 'content': prompt5}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Get the result from the stream\n",
    "    result2 = output(stream)\n",
    "    return result2\n",
    "rev = revised(question,irrelevant,replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92422789-bace-46ae-8fdc-ee81495ea1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Part A (10 x 2 = 20 Marks):* \n",
      "1. [What is the primary purpose of data normalization in a database ?] 2 CO1 BL1 \n",
      "2. [Define the term 'quantitative data' within the context of Data Science.] 2 CO1 BL1 \n",
      "3. When should Bies-vriance decomposition be used? 2 CO2 BL2 \n",
      "4. [Explain how big data differs from traditional data sets in terms of volume, variety, and velocity.] 2 CO2 BL2 \n",
      "5. In what situations is PCA (Principal Component Analysis) suitable? 2 CO3 BL2 \n",
      "6. Explain how Linear Discriminant Analysis (LDA) works. 2 CO3 BL2 \n",
      "7. What are Transition Probabilities? 2 CO4 BL1 \n",
      "8. Define the HMM Viterbi algorithm. 2 CO4 BL1 \n",
      "9. [Apply the concepts learned about distributed file systems to design an efficient storage solution for a large dataset.] 2 CO5 BL1 \n",
      "10. [Analyze a given dataset using descriptive statistics to identify any outliers or trends, and interpret their implications.] 2 CO5 BL1 \n",
      "\n",
      "*Part B (5 x 13 = 65 Marks):* \n",
      "11a. Write the FIND-S algorithm and provide an example to trace it. (13) CO1 BL1, BL2 \n",
      "OR \n",
      "11b. [Given a real-world scenario, suggest an appropriate data structure for storing and organizing the data based on its properties.] \n",
      "\n",
      "12a. [Develop a hypothesis about the relationship between two variables in a given dataset, and design an experiment to test this hypothesis using EDA techniques.] \n",
      "OR \n",
      "12b. Using K-NN Algorithm, predict which sport Angelina plays belonging to? \n",
      "\n",
      "13a. Demonstrate the EM algorithm in detail and justify how it overcomes the drawback of K-means. (13) CO3 BL3 \n",
      "OR \n",
      "13b. Explain how Single-Linkage Hierarchical Clustering groups data samples using an example. \n",
      "\n",
      "14a. In a scenario where there are multiple deadlines but none urgent, find out the activity that should be happening? (13) CO4 BL4 \n",
      "OR \n",
      "14b. [Discuss the ethical implications of sharing sensitive data in Data Science, and propose solutions for mitigating potential privacy concerns.] \n",
      "\n",
      "15a. Illustrate Ensemble Learning with an example and its application. (13) CO5 BL3 \n",
      "OR \n",
      "15b. [Design an interactive dashboard using Tableau 2019.2 that effectively visualizes a complex dataset for business decision-making purposes.] \n",
      "\n",
      "*Part C (1 x 15 = 15 Marks):* \n",
      "16a. Given the data in Table, Estimate the Eigen Values and reduce the dimension from 2 to 1 using the Principal Component Analysis (PCA) algorithm. (15) CO3 BL5, BL6 \n",
      "OR \n",
      "16b. Explain how to evaluate a Support Vector Machine (SVM) model and create a hyperplane for classifying data samples. \n"
     ]
    }
   ],
   "source": [
    "#FINAL\n",
    "printer(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917aa8c1-88b6-4094-a212-c47cbff511e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mm.txt', 'w') as file:\n",
    "    file.write(rev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
